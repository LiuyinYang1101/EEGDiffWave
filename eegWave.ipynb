{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91fed86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YLY\\miniconda3\\envs\\torchEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from preprocess import get_data\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from util import rescale, find_max_epoch, print_size\n",
    "from util import training_loss, calc_diffusion_hyperparams\n",
    "\n",
    "from distributed_util import init_distributed, apply_gradient_allreduce, reduce_tensor\n",
    "\n",
    "from models import WaveNet_vocoder as WaveNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63912df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, sub, LOSO, isStandard, purpose):\n",
    "        if purpose==1:\n",
    "            self.Data, _, self.Labels, _, _, _ = get_data(data_path, sub, LOSO, isStandard)\n",
    "\n",
    "        elif purpose ==2:\n",
    "            _, _, _, self.Data, _, self.Labels = get_data(data_path, sub, LOSO, isStandard)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.Data[idx,:,:,:]\n",
    "        label = self.Labels[idx]\n",
    "        return data, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory exp\\ch1_T200_betaT0.02\\logs/checkpoint\n",
      "WaveNet_vocoder Parameters: 0.351368M\n",
      "Successfully loaded model at iteration 65000\n",
      "iteration: 66000 \treduced loss: 0.6088373064994812 \tloss: 0.6088373064994812\n",
      "iteration: 67000 \treduced loss: 0.5860238671302795 \tloss: 0.5860238671302795\n",
      "iteration: 68000 \treduced loss: 0.602728009223938 \tloss: 0.602728009223938\n",
      "iteration: 69000 \treduced loss: 0.6097601652145386 \tloss: 0.6097601652145386\n",
      "iteration: 70000 \treduced loss: 0.6116840839385986 \tloss: 0.6116840839385986\n",
      "model at iteration 70000 is saved\n",
      "iteration: 71000 \treduced loss: 0.6265772581100464 \tloss: 0.6265772581100464\n",
      "iteration: 72000 \treduced loss: 0.6248276233673096 \tloss: 0.6248276233673096\n",
      "iteration: 73000 \treduced loss: 0.6022388935089111 \tloss: 0.6022388935089111\n",
      "iteration: 74000 \treduced loss: 0.5772449374198914 \tloss: 0.5772449374198914\n",
      "iteration: 75000 \treduced loss: 0.6063990592956543 \tloss: 0.6063990592956543\n",
      "model at iteration 75000 is saved\n",
      "iteration: 76000 \treduced loss: 0.5989986062049866 \tloss: 0.5989986062049866\n",
      "iteration: 77000 \treduced loss: 0.5924771428108215 \tloss: 0.5924771428108215\n",
      "iteration: 78000 \treduced loss: 0.6042048931121826 \tloss: 0.6042048931121826\n",
      "iteration: 79000 \treduced loss: 0.6310468912124634 \tloss: 0.6310468912124634\n",
      "iteration: 80000 \treduced loss: 0.594009280204773 \tloss: 0.594009280204773\n",
      "model at iteration 80000 is saved\n"
     ]
    }
   ],
   "source": [
    "def train(num_gpus, rank, group_name, output_directory, tensorboard_directory,\n",
    "          ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging,\n",
    "          learning_rate, batch_size_per_gpu):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    num_gpus, rank, group_name:     parameters for distributed training\n",
    "    output_directory (str):         save model checkpoints to this path\n",
    "    tensorboard_directory (str):    save tensorboard events to this path\n",
    "    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded; \n",
    "                                    automitically selects the maximum iteration if 'max' is selected\n",
    "    n_iters (int):                  number of iterations to train, default is 1M\n",
    "    iters_per_ckpt (int):           number of iterations to save checkpoint, \n",
    "                                    default is 10k, for models with residual_channel=64 this number can be larger\n",
    "    iters_per_logging (int):        number of iterations to save training log, default is 100\n",
    "    learning_rate (float):          learning rate\n",
    "    batch_size_per_gpu (int):       batchsize per gpu, default is 2 so total batchsize is 16 with 8 gpus\n",
    "    \"\"\"\n",
    "\n",
    "    # generate experiment (local) path\n",
    "    local_path = \"ch{}_T{}_betaT{}\".format(wavenet_config[\"res_channels\"], \n",
    "                                           diffusion_config[\"T\"], \n",
    "                                           diffusion_config[\"beta_T\"])\n",
    "    # Create tensorboard logger.\n",
    "    if rank == 0:\n",
    "        tb = SummaryWriter(os.path.join('exp', local_path, tensorboard_directory))\n",
    "\n",
    "    # distributed running initialization\n",
    "    if num_gpus > 1:\n",
    "        init_distributed(rank, num_gpus, group_name, **dist_config)\n",
    "\n",
    "    # Get shared output_directory ready\n",
    "    output_directory = os.path.join('exp', local_path, output_directory)\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        print(\"output directory\", output_directory, flush=True)\n",
    "\n",
    "    # map diffusion hyperparameters to gpu\n",
    "    for key in diffusion_hyperparams:\n",
    "        if key is not \"T\":\n",
    "            diffusion_hyperparams[key] = diffusion_hyperparams[key].cuda()\n",
    "    \n",
    "    # predefine model\n",
    "    net = WaveNet(**wavenet_config).cuda()\n",
    "    print_size(net)\n",
    "\n",
    "    # apply gradient all reduce\n",
    "    if num_gpus > 1:\n",
    "        net = apply_gradient_allreduce(net)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # load checkpoint\n",
    "    if ckpt_iter == 'max':\n",
    "        ckpt_iter = find_max_epoch(output_directory)\n",
    "    if ckpt_iter >= 0:\n",
    "        try:\n",
    "            # load checkpoint file\n",
    "            model_path = os.path.join(output_directory, '{}.pkl'.format(ckpt_iter))\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            \n",
    "            # feed model dict and optimizer state\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            print('Successfully loaded model at iteration {}'.format(ckpt_iter))\n",
    "        except:\n",
    "            ckpt_iter = -1\n",
    "            print('No valid checkpoint model found, start training from initialization.')\n",
    "    else:\n",
    "        ckpt_iter = -1\n",
    "        print('No valid checkpoint model found, start training from initialization.')\n",
    "\n",
    "    # training\n",
    "    n_iter = ckpt_iter + 1\n",
    "    while n_iter < n_iters + 1:\n",
    "        for sub in range(9):\n",
    "            data_path = 'C:/Users/YLY/Documents/bci-comp-iv2a/rawData/'\n",
    "            LOSO = False\n",
    "            isStandard = True\n",
    "\n",
    "            data_train = CustomDataset(data_path, sub, LOSO, isStandard, 1)\n",
    "            train, valid = random_split(data_train,[np.floor(len(data_train)*0.8).astype(int),(len(data_train)-np.floor(len(data_train)*0.8)).astype(int)])\n",
    "            test_dataset = CustomDataset(data_path, sub, LOSO, isStandard, 2)\n",
    "            train_loader = DataLoader(\n",
    "                train, batch_size=64, shuffle=False\n",
    "            )\n",
    "            vali_loader = DataLoader(\n",
    "                valid, batch_size=64, shuffle=False\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=64, shuffle=False\n",
    "            )\n",
    "            # load training data\n",
    "            #print('Data loaded')\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                eeg, labels = data[0].squeeze(1).cuda(), data[1].type(torch.LongTensor).cuda()\n",
    "                # load audio and mel spectrogram\n",
    "                #mel_spectrogram = mel_spectrogram.cuda()\n",
    "                #audio = audio.unsqueeze(1).cuda()\n",
    "\n",
    "                # back-propagation\n",
    "                optimizer.zero_grad()\n",
    "                X = (labels, eeg.float())\n",
    "                loss = training_loss(net, nn.MSELoss(), X, diffusion_hyperparams)\n",
    "                #print(loss)\n",
    "                if num_gpus > 1:\n",
    "                    reduced_loss = reduce_tensor(loss.data, num_gpus).item()\n",
    "                else:\n",
    "                    reduced_loss = loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # output to log\n",
    "                # note, only do this on the first gpu\n",
    "                if n_iter % iters_per_logging == 0 and rank == 0:\n",
    "                    # save training loss to tensorboard\n",
    "                    print(\"iteration: {} \\treduced loss: {} \\tloss: {}\".format(n_iter, reduced_loss, loss.item()))\n",
    "                    tb.add_scalar(\"Log-Train-Loss\", torch.log(loss).item(), n_iter)\n",
    "                    tb.add_scalar(\"Log-Train-Reduced-Loss\", np.log(reduced_loss), n_iter)\n",
    "\n",
    "                # save checkpoint\n",
    "                if n_iter > 0 and n_iter % iters_per_ckpt == 0 and rank == 0:\n",
    "                    checkpoint_name = '{}.pkl'.format(n_iter)\n",
    "                    torch.save({'model_state_dict': net.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict()}, \n",
    "                               os.path.join(output_directory, checkpoint_name))\n",
    "                    print('model at iteration %s is saved' % n_iter)\n",
    "\n",
    "                n_iter += 1\n",
    "\n",
    "    # Close TensorBoard.\n",
    "    if rank == 0:\n",
    "        tb.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parse configs. Globals nicer in this case\n",
    "    with open('configure.json') as f:\n",
    "        data = f.read()\n",
    "    config = json.loads(data)\n",
    "    train_config            = config[\"train_config\"]        # training parameters\n",
    "    global dist_config\n",
    "    dist_config             = config[\"dist_config\"]         # to initialize distributed training\n",
    "    global wavenet_config\n",
    "    wavenet_config          = config[\"wavenet_config\"]      # to define wavenet\n",
    "    global diffusion_config\n",
    "    diffusion_config        = config[\"diffusion_config\"]    # basic hyperparameters\n",
    "    global trainset_config\n",
    "    trainset_config         = config[\"trainset_config\"]     # to load trainset\n",
    "    global diffusion_hyperparams \n",
    "    diffusion_hyperparams   = calc_diffusion_hyperparams(**diffusion_config)  # dictionary of all diffusion hyperparameters\n",
    "\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    train(1, 0, \"test\", **train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a02e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20074628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7ab5ce5dd3ede4a5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7ab5ce5dd3ede4a5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%tensorboard --logdir=/exp/ch1_T200_betaT0.02/logs/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
