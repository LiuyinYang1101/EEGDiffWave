{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91fed86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YLY\\miniconda3\\envs\\torchEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from preprocess import get_data\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from util import rescale, find_max_epoch, print_size\n",
    "from util import training_loss, calc_diffusion_hyperparams\n",
    "\n",
    "from distributed_util import init_distributed, apply_gradient_allreduce, reduce_tensor\n",
    "\n",
    "from models2 import EEGWav_diff as WaveNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63912df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, sub, LOSO, isStandard, purpose):\n",
    "        if purpose==1:\n",
    "            self.Data, _, self.Labels, _, _, _ = get_data(data_path, sub, LOSO, isStandard)\n",
    "\n",
    "        elif purpose ==2:\n",
    "            _, _, _, self.Data, _, self.Labels = get_data(data_path, sub, LOSO, isStandard)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.Data[idx,:,:,:]\n",
    "        label = self.Labels[idx]\n",
    "        return data, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c72ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory exp\\ch128_T200_betaT0.02\\logs/checkpoint\n",
      "EEGWav_diff Parameters: 5.939790M\n",
      "No valid checkpoint model found, start training from initialization.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    161\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m train(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_config)\n",
      "Cell \u001b[1;32mIn [5], line 88\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_gpus, rank, group_name, output_directory, tensorboard_directory, ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging, learning_rate, batch_size_per_gpu)\u001b[0m\n\u001b[0;32m     86\u001b[0m data_train \u001b[38;5;241m=\u001b[39m CustomDataset(data_path, sub, LOSO, isStandard, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     87\u001b[0m train, valid \u001b[38;5;241m=\u001b[39m random_split(data_train,[np\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;28mlen\u001b[39m(data_train)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m),(\u001b[38;5;28mlen\u001b[39m(data_train)\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;28mlen\u001b[39m(data_train)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.8\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)])\n\u001b[1;32m---> 88\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOSO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misStandard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     90\u001b[0m     train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     92\u001b[0m vali_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     93\u001b[0m     valid, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     94\u001b[0m )\n",
      "Cell \u001b[1;32mIn [2], line 8\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[1;34m(self, data_path, sub, LOSO, isStandard, purpose)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mData, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLabels, _, _, _ \u001b[38;5;241m=\u001b[39m get_data(data_path, sub, LOSO, isStandard)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m purpose \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m     _, _, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mData, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLabels \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOSO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misStandard\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\eegWave\\preprocess.py:159\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(path, subject, LOSO, isStandard)\u001b[0m\n\u001b[0;32m    157\u001b[0m     path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(subject\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    158\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m load_data(path, subject\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 159\u001b[0m     X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Prepare training data \t\u001b[39;00m\n\u001b[0;32m    162\u001b[0m N_tr, N_ch, _ \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape \n",
      "File \u001b[1;32m~\\Documents\\eegWave\\preprocess.py:101\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(data_path, subject, training, all_trials)\u001b[0m\n\u001b[0;32m     98\u001b[0m window_Length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m250\u001b[39m \n\u001b[0;32m    100\u001b[0m class_return \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_tests)\n\u001b[1;32m--> 101\u001b[0m data_return \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_Length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m NO_valid_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(num_gpus, rank, group_name, output_directory, tensorboard_directory,\n",
    "          ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging,\n",
    "          learning_rate, batch_size_per_gpu):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    num_gpus, rank, group_name:     parameters for distributed training\n",
    "    output_directory (str):         save model checkpoints to this path\n",
    "    tensorboard_directory (str):    save tensorboard events to this path\n",
    "    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded; \n",
    "                                    automitically selects the maximum iteration if 'max' is selected\n",
    "    n_iters (int):                  number of iterations to train, default is 1M\n",
    "    iters_per_ckpt (int):           number of iterations to save checkpoint, \n",
    "                                    default is 10k, for models with residual_channel=64 this number can be larger\n",
    "    iters_per_logging (int):        number of iterations to save training log, default is 100\n",
    "    learning_rate (float):          learning rate\n",
    "    batch_size_per_gpu (int):       batchsize per gpu, default is 2 so total batchsize is 16 with 8 gpus\n",
    "    \"\"\"\n",
    "\n",
    "    # generate experiment (local) path\n",
    "    local_path = \"ch{}_T{}_betaT{}\".format(wavenet_config[\"res_channels\"], \n",
    "                                           diffusion_config[\"T\"], \n",
    "                                           diffusion_config[\"beta_T\"])\n",
    "    # Create tensorboard logger.\n",
    "    if rank == 0:\n",
    "        tb = SummaryWriter(os.path.join('exp', local_path, tensorboard_directory))\n",
    "\n",
    "    # distributed running initialization\n",
    "    if num_gpus > 1:\n",
    "        init_distributed(rank, num_gpus, group_name, **dist_config)\n",
    "\n",
    "    # Get shared output_directory ready\n",
    "    output_directory = os.path.join('exp', local_path, output_directory)\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        print(\"output directory\", output_directory, flush=True)\n",
    "\n",
    "    # map diffusion hyperparameters to gpu\n",
    "    for key in diffusion_hyperparams:\n",
    "        if key is not \"T\":\n",
    "            diffusion_hyperparams[key] = diffusion_hyperparams[key].cuda()\n",
    "    \n",
    "    # predefine model\n",
    "    net = WaveNet(**wavenet_config).cuda()\n",
    "    print_size(net)\n",
    "\n",
    "    # apply gradient all reduce\n",
    "    if num_gpus > 1:\n",
    "        net = apply_gradient_allreduce(net)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # load checkpoint\n",
    "    if ckpt_iter == 'max':\n",
    "        ckpt_iter = find_max_epoch(output_directory)\n",
    "    if ckpt_iter >= 0:\n",
    "        try:\n",
    "            # load checkpoint file\n",
    "            model_path = os.path.join(output_directory, '{}.pkl'.format(ckpt_iter))\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            \n",
    "            # feed model dict and optimizer state\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            print('Successfully loaded model at iteration {}'.format(ckpt_iter))\n",
    "        except:\n",
    "            ckpt_iter = -1\n",
    "            print('No valid checkpoint model found, start training from initialization.')\n",
    "    else:\n",
    "        ckpt_iter = -1\n",
    "        print('No valid checkpoint model found, start training from initialization.')\n",
    "\n",
    "    # training\n",
    "    n_iter = ckpt_iter + 1\n",
    "    while n_iter < n_iters + 1:\n",
    "        for sub in range(9):\n",
    "     \n",
    "            data_path = trainset_config['data_path']\n",
    "            LOSO = False\n",
    "            isStandard = True\n",
    "\n",
    "            data_train = CustomDataset(data_path, sub, LOSO, isStandard, 1)\n",
    "            train, valid = random_split(data_train,[np.floor(len(data_train)*0.8).astype(int),(len(data_train)-np.floor(len(data_train)*0.8)).astype(int)])\n",
    "            test_dataset = CustomDataset(data_path, sub, LOSO, isStandard, 2)\n",
    "            train_loader = DataLoader(\n",
    "                train, batch_size=64, shuffle=False\n",
    "            )\n",
    "            vali_loader = DataLoader(\n",
    "                valid, batch_size=64, shuffle=False\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=64, shuffle=False\n",
    "            )\n",
    "            # load training data\n",
    "            #print('Data loaded')\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                eeg, labels = data[0].squeeze(1).cuda(), data[1].type(torch.LongTensor).cuda()\n",
    "                # load audio and mel spectrogram\n",
    "                #mel_spectrogram = mel_spectrogram.cuda()\n",
    "                #audio = audio.unsqueeze(1).cuda()\n",
    "\n",
    "                # back-propagation\n",
    "                optimizer.zero_grad()\n",
    "                X = (labels, eeg.float())\n",
    "                loss = training_loss(net, nn.MSELoss(), X, diffusion_hyperparams)\n",
    "                #print(loss)\n",
    "                if num_gpus > 1:\n",
    "                    reduced_loss = reduce_tensor(loss.data, num_gpus).item()\n",
    "                else:\n",
    "                    reduced_loss = loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # output to log\n",
    "                # note, only do this on the first gpu\n",
    "                if n_iter % iters_per_logging == 0 and rank == 0:\n",
    "                    # save training loss to tensorboard\n",
    "                    print(\"iteration: {} \\treduced loss: {} \\tloss: {}\".format(n_iter, reduced_loss, loss.item()))\n",
    "                    tb.add_scalar(\"Log-Train-Loss\", torch.log(loss).item(), n_iter)\n",
    "                    tb.add_scalar(\"Log-Train-Reduced-Loss\", np.log(reduced_loss), n_iter)\n",
    "\n",
    "                # save checkpoint\n",
    "                if n_iter > 0 and n_iter % iters_per_ckpt == 0 and rank == 0:\n",
    "                    checkpoint_name = '{}.pkl'.format(n_iter)\n",
    "                    torch.save({'model_state_dict': net.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict()}, \n",
    "                               os.path.join(output_directory, checkpoint_name))\n",
    "                    print('model at iteration %s is saved' % n_iter)\n",
    "\n",
    "                n_iter += 1\n",
    "\n",
    "    # Close TensorBoard.\n",
    "    if rank == 0:\n",
    "        tb.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parse configs. Globals nicer in this case\n",
    "    with open('config-deep.json') as f:\n",
    "        data = f.read()\n",
    "    config = json.loads(data)\n",
    "    train_config            = config[\"train_config\"]        # training parameters\n",
    "    global dist_config\n",
    "    dist_config             = config[\"dist_config\"]         # to initialize distributed training\n",
    "    global wavenet_config\n",
    "    wavenet_config          = config[\"wavenet_config\"]      # to define wavenet\n",
    "    global diffusion_config\n",
    "    diffusion_config        = config[\"diffusion_config\"]    # basic hyperparameters\n",
    "    global trainset_config\n",
    "    trainset_config         = config[\"trainset_config\"]     # to load trainset\n",
    "    global diffusion_hyperparams \n",
    "    diffusion_hyperparams   = calc_diffusion_hyperparams(**diffusion_config)  # dictionary of all diffusion hyperparameters\n",
    "\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    train(1, 0, \"test\", **train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20074628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%tensorboard --logdir=/exp/ch1_T200_betaT0.02/logs/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb14e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
